#import "/typ/templates/blog.typ": *
#show: main.with(
  title: "Rationality",
  desc: "",
  date: "2025-12-20T11:26:04-05:00",
  tags: ("musings",),
)


#definition(name: "Rationalism", number: false)[
  A reasoning framework for estimating the truth. It cannot answer questions about what a "good" decision or "bad" decision by itself. For that you need a set of values.
] <Rationalism>
#definition(name: "Values", number: false)[
  Your _objective function_. There is no "true" objective function, no formula derived from the laws of physics that tells us how to compute the utility of a human life, so that we can maximize it in aggregate. Nonetheless, there are still some properties your set of values should strive for.
  + Self-Consistency: No subset of values can be used to deduce another value is false.
  + Transitivity: A > B and B > C implies A > C.
  + Comparability: Any two decisions can be scored.
] <Values>

Another way to think about it is #link(<Values>, "Values") are your axioms, and #link(<Rationalism>, "Rationalism") is constraint propagation.

I find this distinction important because the two concepts can be easily mixed up, and separating them allows answering a few questions. 

== Can two rational people "agree to disagree"?
The thinking here is that two rationalists should not be able to hold opposing viewpoints on the same issue given the same evidence. However, that's not true. The two rationalists could simply have different #link(<Values>, "Values"), and therefore the "rational" choice for both of them need not be the same. The key point is rationality is not by itself a value system. It can't be used to prove your value axioms are true.

== Is "being positive" irrational?
I've always found this advice kind of troubling because it seems to imply you should lie to yourself about how good things are. However, that's not necessarily the case. Imagine you thought you had slim odds of doing ground-breaking research, and that made you pretty sad. Consider the following two approaches to positivity.
+ I tell myself that instead of slim odds of doing great research, I have massive odds! 
+ I tell myself, isn't it great that I'm in a position where I can spend my time investigating the things that interest me most? Maybe I won't change the world, but I'll push the field forward while pursuing my passion.

Approach 1 manipulated the truth. Approach 2 updated our #link(<Values>, "Values"). Both approaches led to a better assessment of how things are, but Approach 2 didn't require lying to ourselves. 

So "being positive" doesn't require being irrational. That's great, because Approach 1 is not stable. If the truth scares you enough that you need to lie to yourself to proceed, then you should seriously consider whether it's worth it to proceed. In the example, imagine the researcher was really only interested in producing ground-breaking work with high odds. If that really wasn't possible to the best of anyone's knowledge, he's probably better off quitting. Maybe he'd get lucky and publish a breakthrough if he persisted, but most likely he wouldn't and would spend his career feeling disappointed.

== Can a Rational person be an optimist?
This seems almost like a contradiction at first glance. The truth doesn't care what makes you happy. The truth just _is_. So then, is it really possible to be rational and consistently feel happy, or does this require some kind of self-deception?

It doesn't require self-deception. This is because what is "good" and "positive" is an inherently value-based thing. The truth might not care what makes you happy, but you get to decide what makes you happy, as least to some extent. You can choose to derive happiness from all that you have, or to derive sadness from all that you lack. Neither is "irrational", they are both two different weightings of the same situation. Hence, the optimist need not be irrational.

== Can you make a Church of Rationality?
I was thinking about this when talking to a friend. I mentioned that I found it kind of unfortunate that while there were many communities dedicated to following a religion, there wasn't anything for secular people. There's no major church of rationality, humanism, or atheism despite the large number of secular people world-wide. Why not? There's probably a dozen reasons, but the one that stands out to me is that a religion gives its people a shared set of values. It rejects my claim that there is no true objective function by giving its people a specialized set of rules and duties. The secular followings I mentioned make no such attempt.